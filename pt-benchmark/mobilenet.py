#!/usr/bin/env python
from __future__ import print_function
import os
import torch
import torch.distributed as dist
from torch.multiprocessing import Process
import torchvision.datasets as datasets
from torchvision import transforms, utils
from random import Random
import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.multiprocessing as mp

import time
import math

# # Hyper-parameters
learning_rate = 0.01


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('-n', '--nodes', default=1, type=int, metavar='N')
    parser.add_argument('-g', '--gpus', default=1, type=int,
                        help='number of gpus per node')
    parser.add_argument('-nr', '--nr', default=0, type=int,
                        help='ranking within the nodes')
    parser.add_argument('-epochs', default=1, type=int, metavar='N',
                        help='number of total epochs to run')
    parser.add_argument('-batch_size', default=32, type=int, metavar='N',
                        help='number of batch size')
    parser.add_argument('-expname', default='default', type=str, metavar='N',
                        help='name of running ')
    parser.add_argument('-datadir', default='./data/', type=str, metavar='N',
                        help='name of data directory')
    args = parser.parse_args()
    
    # total number of processes to run
    args.world_size = args.gpus * args.nodes    
    
    #IP address for process 0 so that all proc can sync up at first
    os.environ['MASTER_ADDR'] = '10.143.3.3'              
    os.environ['MASTER_PORT'] = '2224'      
    # each process run train(i, args)                
    mp.spawn(train, nprocs=args.gpus, args=(args,))       
        
    
def conv_bn(inp, oup, stride):
    return nn.Sequential(
        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),
        nn.BatchNorm2d(oup),
        nn.ReLU6(inplace=True)
    )

def make_divisible(x, divisible_by=8):
    import numpy as np
    return int(np.ceil(x * 1. / divisible_by) * divisible_by)

def conv_1x1_bn(inp, oup):
    return nn.Sequential(
        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),
        nn.BatchNorm2d(oup),
        nn.ReLU6(inplace=True)
    )

class InvertedResidual(nn.Module):
    def __init__(self, inp, oup, stride, expand_ratio):
        super(InvertedResidual, self).__init__()
        self.stride = stride
        assert stride in [1, 2]

        hidden_dim = int(inp * expand_ratio)
        self.use_res_connect = self.stride == 1 and inp == oup

        if expand_ratio == 1:
            self.conv = nn.Sequential(
                # dw
                nn.Conv2d(hidden_dim, hidden_dim, 1, stride, 1, groups=hidden_dim, bias=False),
                nn.BatchNorm2d(hidden_dim),
                nn.ReLU6(inplace=True),
                # pw-linear
                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),
                nn.BatchNorm2d(oup),
            )
        else:
            self.conv = nn.Sequential(
                # pw
                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),
                nn.BatchNorm2d(hidden_dim),
                nn.ReLU6(inplace=True),
                # dw
                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),
                nn.BatchNorm2d(hidden_dim),
                nn.ReLU6(inplace=True),
                # pw-linear
                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),
                nn.BatchNorm2d(oup),
            )

    def forward(self, x):
        if self.use_res_connect:
            return x + self.conv(x)
        else:
            return self.conv(x)

class MobileNetV2(nn.Module):
    def __init__(self, n_class=1000, input_size=224, width_mult=1.):
        super(MobileNetV2, self).__init__()
        block = InvertedResidual
        input_channel = 32
        last_channel = 1280
        interverted_residual_setting = [
            # t, c, n, s
            [1, 16, 1, 1],
            [6, 24, 2, 2],
            [6, 32, 3, 2],
            [6, 64, 4, 2],
            [6, 96, 3, 1],
            [6, 160, 3, 2],
            [6, 320, 1, 1],
        ]

        # building first layer
        assert input_size % 32 == 0
        # input_channel = make_divisible(input_channel * width_mult)  # first channel is always 32!
        self.last_channel = make_divisible(last_channel * width_mult) if width_mult > 1.0 else last_channel
        self.features = [conv_bn(1, input_channel, 2)]
        # building inverted residual blocks
        for t, c, n, s in interverted_residual_setting:
            output_channel = make_divisible(c * width_mult) if t > 1 else c
            for i in range(n):
                if i == 0:
                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))
                else:
                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))
                input_channel = output_channel
        # building last several layers
        self.features.append(conv_1x1_bn(input_channel, self.last_channel))
        # make it nn.Sequential
        self.features = nn.Sequential(*self.features)

        # building classifier
        self.classifier = nn.Linear(self.last_channel, n_class)

        self._initialize_weights()

    def forward(self, x):
        x = self.features(x)
        x = x.mean(3).mean(2)
        x = self.classifier(x)
        return x

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2. / n))
                if m.bias is not None:
                    m.bias.data.zero_()
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()
            elif isinstance(m, nn.Linear):
                n = m.weight.size(1)
                m.weight.data.normal_(0, 0.01)
                m.bias.data.zero_()
                



""" Distributed Synchronous SGD Example """
def train(gpu, args):
    
    #  global rank of the process (one proc per gpu)
    rank = args.nr * args.gpus + gpu
    
    
    # used nccl backend (fastest)             
    dist.init_process_group(                                   
        backend='nccl',                                         
        init_method='env://',                                   
        world_size=args.world_size,                              
        rank=rank                                               
    )       
    
    
    torch.manual_seed(0)
    model = MobileNetV2(width_mult=1)
    
    torch.cuda.set_device(gpu)
    model.cuda(gpu)
    
    
    criterion = nn.CrossEntropyLoss().cuda(gpu)
    
    # Create a SGD optimizer for gradient descent
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    
    
    # wrap the model
    model = nn.parallel.DistributedDataParallel(model,
                                                device_ids=[gpu])
    
    

    # MNIST
    train_dataset  = datasets.MNIST(args.datadir, train=True, download=True,
                             transform=transforms.Compose([
                                 transforms.ToTensor(),
                                 transforms.Normalize((0.1307,), (0.3081,))
                             ]))
    # Imagnet
    #train_dataset  = datasets.ImageNet('./data', train=True,
    #                         transform=transforms.Compose([
    #                             transforms.Resize(256),
    #                             transforms.RandomCrop(224),
    #                             transforms.RandomHorizontalFlip(),
    #                             transforms.ToTensor(),
    #                             transforms.Normalize((0.485, 0.456, 0.406),
    #                                                    (0.229, 0.224, 0.225))
    #                         ]))
    
    
    # makes sure that each process gets a different slice of the training data.
    # Use the nn.utils.data.DistributedSampler instead of shuffling the usual way.
    train_sampler = torch.utils.data.distributed.DistributedSampler(
        train_dataset,
        num_replicas=args.world_size,
        rank=rank
    )


    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                               batch_size=args.batch_size,
                                               shuffle=False,
                                               num_workers=0,
                                               pin_memory=True,
                                               sampler=train_sampler)

    
    total_step = len(train_loader)  
    start_time = time.perf_counter()
         
    for epoch in range(args.epochs):
        
        for i, (images, labels) in enumerate(train_loader):
            
            images = images.cuda(non_blocking=True)
            labels = labels.cuda(non_blocking=True)
            outputs = model(images) # forward pass input through the network
            loss = criterion(outputs, labels)
            
            
            # Backward and optimize
            optimizer.zero_grad() # zeroes the gradient buffers to reset the gradient computed by last images batch
            loss.backward()
            optimizer.step()
            if (i + 1) % 100 == 0 and gpu == 0:
                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(
                    epoch + 1, 
                    args.epochs, 
                    i + 1, 
                    total_step,
                    loss.item())
                   )
            #if idx > 5:
            #    print("NORMAL END")
            #    break
                
    if gpu == 0:
        stop_time = time.perf_counter()

        # logout
        print('  duration (via time.perf_counter()): %f (%f - %f)' % (stop_time - start_time, stop_time, start_time))
        print('  Throughput [image/sec] : %f  ' % (args.batch_size*total_step*args.epochs/(stop_time - start_time) ) )
            
        # save log
        os.makedirs("./log", exist_ok=True)
        with open("./log/"+args.expname+'.txt', "w") as f:
          f.write(str(stop_time - start_time)+','+str(args.batch_size*total_step*args.epochs/(stop_time - start_time)))
            


if __name__ == "__main__":
    main()
    
